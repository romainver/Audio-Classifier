{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.io import wavfile\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, LSTM\n",
    "from keras.layers import Dropout, Dense, TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tqdm import tqdm\n",
    "from python_speech_features import mfcc\n",
    "%matplotlib inline\n",
    "#CLass used to tweak hyperparameters easily\n",
    "class Config:\n",
    "    def __init__(self, mode='conv', nfilt=26, nfeat=13, nfft=512, rate=16000):\n",
    "        #mode is the neural network we are going to use (between convolutional and recurrent)\n",
    "        self.mode = mode\n",
    "        #nfilt is the nb of filters used for the mel filter bank\n",
    "        self.nfilt = nfilt\n",
    "        #nfeat is the nb of filter kept for the mel cepstrum coefficient\n",
    "        self.nfeat = nfeat\n",
    "        #nfft is the nb of points inside a window\n",
    "        self.nfft = nfft\n",
    "        #rate is the sampling rate in Hz\n",
    "        self.rate = rate\n",
    "        #number of sampels in a window\n",
    "        self.step = int(rate/10)\n",
    "def generate_features():\n",
    "    X = []\n",
    "    y = []\n",
    "    _min, _max = float('inf'), float('-inf')\n",
    "    for _ in tqdm(range(n_samples)):\n",
    "        #pick randomly a class based on the mean record length of each class\n",
    "        rand_class = np.random.choice(class_dist.index, p = prob_dist)\n",
    "        #pick randomly a file from the class chosen \n",
    "        file = np.random.choice(df[df.label == rand_class].index)\n",
    "        #load the file\n",
    "        rate, wav = wavfile.read('/home/romain/TF Notebooks/Audio Classifier/clean/'+file)\n",
    "        #get the label of the chosen file\n",
    "        label = df.at[file,'label']\n",
    "        #pick the index of where to start the window in our sample\n",
    "        rand_index = np.random.randint(0,wav.shape[0]-config.step)\n",
    "        #get all the data points within the window\n",
    "        sample = wav[rand_index:rand_index + config.step]\n",
    "        #convert sample to mfcc sample\n",
    "        X_sample = mfcc(sample, rate, numcep=config.nfeat,nfilt=config.nfilt, nfft=config.nfft).T\n",
    "        #compare min and max value of current sample with min and max values of all samples\n",
    "        _min = min(np.amin(X_sample), _min)\n",
    "        _max = max(np.amax(X_sample), _max)\n",
    "        #add feature to global array, transpose the matrix or not depending on the type of NN used\n",
    "        X.append(X_sample if config.mode == 'conv' else X_sample.T)\n",
    "        #add number label corresponding to string label to global array\n",
    "        y.append(classes.index(label))\n",
    "    #convert arrays to numpy arrays\n",
    "    X,y = np.array(X), np.array(y)\n",
    "    #normalize the samples between 0 and 1 values\n",
    "    X = (X - _min) / (_max - _min)\n",
    "    if config.mode=='conv':\n",
    "        #Add the 1 pixel for CNN (always put a constant 1 in the input for CNN)\n",
    "        X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "    elif config.mode == 'time':\n",
    "        X = X.reshape(X.shape[0],X.shape[1],X.shape[2])\n",
    "        #OneHot encoding of the label for the cost function\n",
    "    y = to_categorical(y, num_classes=10)\n",
    "    return X,y\n",
    "\n",
    "def get_cnn_model():\n",
    "    #allows to stack up layers without specifying which ones are connected to which ones\n",
    "    model = Sequential()\n",
    "    #Convlayers\n",
    "    model.add(Conv2D(16, (3,3), activation='relu', strides=(1,1), padding='same', input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (3,3), activation='relu', strides=(1,1), padding='same'))\n",
    "    model.add(Conv2D(64, (3,3), activation='relu', strides=(1,1), padding='same'))\n",
    "    model.add(Conv2D(128, (3,3), activation='relu', strides=(1,1), padding='same'))\n",
    "    #pooling layer\n",
    "    model.add(MaxPool2D((2,2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    #dense layers\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dense(10,activation='softmax'))\n",
    "    model.summary()\n",
    "    #optimizer\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "def get_rnn_model():\n",
    "    #allows to stack up layers without specifying which ones are connected to which ones\n",
    "    model = Sequential()\n",
    "    #CNN layers\n",
    "    model.add(LSTM(128, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(TimeDistributed(Dense(64, activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(32, activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(16, activation='relu')))\n",
    "    model.add(TimeDistributed(Dense(8, activation='relu')))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10,activation='softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "df = pd.read_csv('/home/romain/TF Notebooks/Audio Classifier/instruments.csv')\n",
    "df.set_index('fname', inplace=True)\n",
    "\n",
    "for f in df.index:\n",
    "    rate, signal = wavfile.read('/home/romain/TF Notebooks/Audio Classifier/clean/'+f)\n",
    "    df.at[f, 'length'] = signal.shape[0]/rate\n",
    "# sum up the lenght of all samples, divide it by window size to get number of total windows, and multiply by 2 to have enough samples\n",
    "n_samples = 2 * int(df['length'].sum() / 0.1)\n",
    "#Create a list of all possible classes\n",
    "classes = list(np.unique(df.label))\n",
    "#get the mean length of a record for each classes\n",
    "class_dist = df.groupby(['label'])['length'].mean()\n",
    "#convert the mean length to a float between 0 and 1\n",
    "prob_dist = class_dist / class_dist.sum()\n",
    "#pick randomly a class based on the mean record length of each class\n",
    "choices = np.random.choice(class_dist.index, p= prob_dist)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26410/26410 [00:54<00:00, 487.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 13, 9, 16)         160       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 13, 9, 32)         4640      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 9, 64)         18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 9, 128)        73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               393344    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 499,402\n",
      "Trainable params: 499,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "config = Config(mode='conv')\n",
    "# get the preprocessed features and labels\n",
    "X,y = generate_features()\n",
    "if config.mode == 'conv':\n",
    "    #convert OneHot back to normal\n",
    "    y_flat = np.argmax(y, axis=1)\n",
    "    #shape parameter for CNN model (drop the nb of samples from the shape)\n",
    "    input_shape = (X.shape[1],X.shape[2],1)\n",
    "    #get the model\n",
    "    model = get_cnn_model()\n",
    "elif config.mode == 'time':\n",
    "    #convert OneHot back to normal\n",
    "    y_flat = np.argmax(y, axis=1)\n",
    "    #shape parameter for CNN model (drop the nb of samples from the shape)\n",
    "    input_shape = (X.shape[1],X.shape[2])\n",
    "    #get the model\n",
    "    model = get_rnn_model()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23769 samples, validate on 2641 samples\n",
      "Epoch 1/10\n",
      "23769/23769 [==============================] - 27s 1ms/step - loss: 0.9971 - acc: 0.6453 - val_loss: 0.5215 - val_acc: 0.8194\n",
      "Epoch 2/10\n",
      "23769/23769 [==============================] - 28s 1ms/step - loss: 0.4913 - acc: 0.8314 - val_loss: 0.3507 - val_acc: 0.8872\n",
      "Epoch 3/10\n",
      "23769/23769 [==============================] - 27s 1ms/step - loss: 0.3548 - acc: 0.8750 - val_loss: 0.2689 - val_acc: 0.9057\n",
      "Epoch 4/10\n",
      "23769/23769 [==============================] - 27s 1ms/step - loss: 0.2806 - acc: 0.9009 - val_loss: 0.2240 - val_acc: 0.9193\n",
      "Epoch 5/10\n",
      "23769/23769 [==============================] - 28s 1ms/step - loss: 0.2361 - acc: 0.9170 - val_loss: 0.1862 - val_acc: 0.9303\n",
      "Epoch 6/10\n",
      "23769/23769 [==============================] - 29s 1ms/step - loss: 0.2011 - acc: 0.9306 - val_loss: 0.2069 - val_acc: 0.9265\n",
      "Epoch 7/10\n",
      "23769/23769 [==============================] - 29s 1ms/step - loss: 0.1835 - acc: 0.9342 - val_loss: 0.1578 - val_acc: 0.9466\n",
      "Epoch 8/10\n",
      "23769/23769 [==============================] - 38s 2ms/step - loss: 0.1651 - acc: 0.9435 - val_loss: 0.1678 - val_acc: 0.9474\n",
      "Epoch 9/10\n",
      "23769/23769 [==============================] - 36s 2ms/step - loss: 0.1539 - acc: 0.9461 - val_loss: 0.1600 - val_acc: 0.9459\n",
      "Epoch 10/10\n",
      "23769/23769 [==============================] - 38s 2ms/step - loss: 0.1459 - acc: 0.9488 - val_loss: 0.1424 - val_acc: 0.9512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9e942e7a90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the weight of each classes\n",
    "class_weight = compute_class_weight('balanced',np.unique(y_flat), y_flat)\n",
    "#fit the model\n",
    "model.fit(X, y, epochs=10, batch_size=32, shuffle = True, class_weight = class_weight, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation, testing set with confusion matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
